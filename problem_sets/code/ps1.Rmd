---
title: "PS1 - Econometrics  I"
author: "Gustavo Henrique and Bruno  Tonholo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem Set 1
### Question 1
```{r}
# Setting objects
c = matrix(c(12547, 4818, 3440, 8583, 6657, 4990, 14493, 7990, 6139, 7043, 3758, 1487, 13238, 11033, 4921),
           nrow = 5,
           ncol = 3, 
           byrow = TRUE)
p = c(1.05, 1.04, 1.1)
r = c(1.1, 1.09, 1.14)
alpha = 0.18

# Estimating beta using MME
beta_1 = c()
for (i in 1:nrow(c)) {
  c_1 = c[i, 1]
  c_2 = c[i, 2]
  p_1 = p[1]
  r_2 = r[2]
  beta_1[i] = (p_1*c_1^(-alpha))/(r_2*c_2^(-alpha))
}
beta_hat_1 = sum(beta_1)/length(beta_1)
beta_2 = c()
for (i in 1:nrow(c)) {
  c_2 = c[i, 2]
  c_3 = c[i, 3]
  p_2 = p[2]
  r_3 = r[3]
  beta_2[i] = (p_2*c_2^(-alpha))/(r_3*c_3^(-alpha))
}
beta_hat_2 = sum(beta_2)/length(beta_2)

# Beta estimates
print(c(beta_hat_1, beta_hat_2))
```

### Question 2
Notice that
$$ J(\beta) = \beta'A\beta \\ = \begin{pmatrix} \beta_1 & \beta_2 \end{pmatrix} \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix} \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix} \\
= \sum_{i=1}^2\sum_{j=1}^2\beta_i\beta_ja_{ij} \\ = \beta_1^2a_{11} + \beta_2^2a_{22} + \beta_1\beta_2a_{21} + \beta_1\beta_2a_{12} $$
Hence, the partial derivative of $J(\beta)$ w. r. t. $\beta_i$ is
$$ \frac{\partial J(\beta)}{\partial \beta_i} = 2\beta_ia_{ii} + \beta_ja_{ij} + \beta_ja_{ij} $$
We can then write the Jacobian matrix $\frac{\partial J(\beta)}{\partial \beta}$ as a vector of these partial derivatives
$$ \frac{\partial J(\beta)}{\partial \beta} = (A + A')\beta $$
Therefore, $\frac{\partial J(\beta)}{\partial \beta} = 2A\beta$ if and only if A is symmetric, i.e. if and only if $a_{ij}=a_{ji}, \forall i \neq j$.

### Question 3
We know that $Cov(e)=\Sigma$, for a random vector $e$. Hence, for some nonstochastic matrix $A \in \mathbb{R}^{k \times n}$, we have
$$ Cov(Ae) = ACov(e)A' = A \Sigma A' $$
by the property of the variance-covariance matrix.

### Question 4
Since $\mathbb{E}[e|z]=0$, by the Law of Iterated Expectations (LIE), we have
$$ \mathbb{E}[z'e]=\mathbb{E}[\mathbb{E}[z'e|z]]=\mathbb{E}[z'\mathbb{E}[e|z]]=0 $$
and
$$ Var(z'e)=\mathbb{E}[z'e(z'e)']=\mathbb{E}[z'ee'z]=\mathbb{E}[\mathbb{E}[z'ee'z|z]]=\mathbb{E}[z'\mathbb{E}[ee'|z]z] $$
Let $\mathbb{E}[ee'|z]=\Sigma$ be the covariance matrix of $e$. Hence, we have
$Var(z'e)=z' \Sigma z$. Therefore,
$$ (z'e) \sim \mathcal{N}(0, z' \Sigma z) $$

### Question 5
Notice 1st that since $x$ is uniformly distributed, we have
$$ f(x) = \begin{cases} 
\frac{1}{b-a}, & \text{if } x \in [a, b] \\
0, & \text{otherwise }
\end{cases} $$
which implies
$$ f'(x)=0 \land f''(x)=0 $$
We know that, assuming i.i.d. draws, we obtain
$$ Bias[\hat{f}(x_0)]=\mathbb{E}[\hat{f}(x_0)]-f(x_0) \approx \frac{h^2}{2}f''(x_0)\int z^2K(z)dz + O(h^3) $$
Therefore, we have
$$ Bias[\hat{f}(x_0)] \approx 2 \cdot 0 \cdot \int z^2K(z)dz + O(h^3) = O(h^3) 
$$

### Question 6
